{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_test=True\n",
    "    frac=0.01#0.02 #1.0#\n",
    "    pretrained=True #False#\n",
    "    model= \"tf_efficientnet_b0\"#\"resnet18\"#\"tf_efficientnet_b0\"#\"resnet18\"\n",
    "    train_bs=4\n",
    "    valid_bs=4#12\n",
    "    test_bs=4\n",
    "    epoch=2\n",
    "    fold=[0]\n",
    "    seed= 2022\n",
    "    lr=2e-4\n",
    "    es=4\n",
    "    device=\"cuda\"\n",
    "\n",
    "    # make fold\n",
    "    make_fold=True\n",
    "    \n",
    "    #train, inference\n",
    "    _train=False#True\n",
    "    _predict=True\n",
    "\n",
    "    #file path\n",
    "    path_train=\"E:/kaggle_data/Jigsaw Rate Severity of Toxic Comments/jigsaw-toxic-comment-classification-challen/train.csv\"\n",
    "    path_test=\"E:/kaggle_data/Jigsaw Rate Severity of Toxic Comments/sample_submission.csv\"\n",
    "    path_test2=\"E:/kaggle_data/Jigsaw Rate Severity of Toxic Comments/comments_to_score.csv\"\n",
    "    path_ref_model=\"bert-base-uncased\"\n",
    "    \n",
    "    # path_train=\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\"\n",
    "    # path_test=\"../input/jigsaw-toxic-severity-rating/sample_submission.csv\"\n",
    "    # path_test2=\"../input/jigsaw-toxic-severity-rating/sample_submission.csv\"\n",
    "    # path_ref_model=\"../input/bert-base-uncased\"\n",
    "\n",
    "    #customize\n",
    "    max_sens=512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "\n",
    "    # Clean some punctutations\n",
    "    data = re.sub('\\n', ' ', data)\n",
    "    # Remove ip address\n",
    "    data = re.sub(r'(([0-9]+\\.){2,}[0-9]+)',' ', data)\n",
    "    \n",
    "    data = re.sub(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', data)\n",
    "    # Replace repeating characters more than 3 times to length of 3\n",
    "    data = re.sub(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', data)\n",
    "    # patterns with repeating characters \n",
    "    data = re.sub(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', data)\n",
    "    data = re.sub(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', data)\n",
    "\n",
    "    # Add space around repeating characters\n",
    "    data = re.sub(' +', ' ', data)\n",
    "    \n",
    "    # Ex) I didn ' t -> I didn't\n",
    "    data = re.sub(\" ' \", \"'\", data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_data():\n",
    "    train=pd.read_csv(CFG.path_train)\n",
    "    train[\"target\"]=(train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0).astype(int)\n",
    "    if CFG.make_fold:\n",
    "        skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=5)\n",
    "        train[\"fold\"]=-1\n",
    "        for i,(_,val_idx) in enumerate(skf.split(train.sample(frac=1.),train.target)):\n",
    "            train.loc[val_idx,\"fold\"]=i \n",
    "    # train.comment_text=train.comment_text.apply(lambda x: clean(x))\n",
    "    return train    \n",
    "\n",
    "def get_data_test():\n",
    "    comments_to_score = pd.read_csv(CFG.path_test2)\n",
    "    comments_to_score.columns=[\"id\",\"comment_text\"]\n",
    "    comments_to_score[\"target\"]=-1\n",
    "    return comments_to_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from cfg import get_cfg\n",
    "CFG=get_cfg()\n",
    "\n",
    "import transformers\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def random_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "random_seed(CFG.seed)\n",
    "\n",
    "class SetiDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, conf=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.labels = df['target'].values\n",
    "        self.transform = transform\n",
    "        self.conf = conf\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.loc[idx, 'comment_text']\n",
    "\n",
    "        bert_sens = tokenizer.encode_plus(\n",
    "                                sentence,\n",
    "                                add_special_tokens = True, \n",
    "                                max_length = CFG.max_sens,#2502, # 上で314に設定しています\n",
    "                                pad_to_max_length = True, \n",
    "                                return_attention_mask = True)\n",
    "        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n",
    "        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n",
    "        token_type_ids = torch.tensor(bert_sens['token_type_ids'], dtype=torch.long)\n",
    "\n",
    "        target = torch.tensor(self.labels[idx],dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'ids': ids,\n",
    "            'mask': mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'targets': target\n",
    "        }\n",
    "\n",
    "def get_dataset(train=None,fold=0):\n",
    "    train_df=train[train.fold!=fold].reset_index(drop=True).sample(frac=CFG.frac)\n",
    "    valid_df=train[train.fold==fold].reset_index(drop=True).sample(frac=CFG.frac)\n",
    "    train_dataset=SetiDataset(train_df)\n",
    "    valid_dataset=SetiDataset(valid_df)\n",
    "    return train_dataset,valid_dataset\n",
    "\n",
    "def get_dataloader(train=None,fold=0):\n",
    "    train_dataset,valid_dataset = get_dataset(train,fold)\n",
    "    train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=CFG.train_bs,num_workers=0,shuffle=True)\n",
    "    valid_loader=torch.utils.data.DataLoader(valid_dataset,batch_size=CFG.valid_bs,num_workers=0,shuffle=False)\n",
    "    return train_loader,valid_loader\n",
    "\n",
    "def get_dataset_test(test=None,fold=0):\n",
    "    test_dataset=SetiDataset(test.sample(frac=1.))\n",
    "    return test_dataset\n",
    "\n",
    "def get_dataloader_test(test=None,fold=0):\n",
    "    test_dataset=get_dataset_test(test=test)\n",
    "    test_loader=torch.utils.data.DataLoader(test_dataset,batch_size=CFG.test_bs,num_workers=0,shuffle=False)\n",
    "    return test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfg import get_cfg\n",
    "CFG=get_cfg()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(CFG.path_ref_model)\n",
    "import transformers\n",
    "\n",
    "def loss_fn(outputs,targets):\n",
    "    # return nn.BCEWithLogitsLoss()(outputs,targets)\n",
    "    return nn.BCEWithLogitsLoss()(outputs,targets)\n",
    "\n",
    "def get_model(classes=1,pretrained=True):\n",
    "    model = transformers.BertForSequenceClassification.from_pretrained(CFG.path_ref_model,num_labels=1)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfg import get_cfg\n",
    "CFG=get_cfg()\n",
    "\n",
    "import numpy as np\n",
    "from model import get_model,loss_fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataloader import get_dataset,get_dataloader\n",
    "import os\n",
    "\n",
    "## Engine\n",
    "from tqdm import tqdm\n",
    "class Engine:\n",
    "    def __init__(self,model,optimizer,scheduler=None):\n",
    "        self.model=model\n",
    "        self.optimizer=optimizer\n",
    "        self.scheduler=scheduler\n",
    "        self.device=CFG.device\n",
    "\n",
    "    def loss_fn(self,outputs,targets):\n",
    "        return nn.BCEWithLogitsLoss()(outputs,targets)        \n",
    "\n",
    "    def train(self,data_loader):\n",
    "        self.model.train()\n",
    "        final_loss=0\n",
    "        for data in tqdm(data_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            ids = data[\"ids\"].to(CFG.device,non_blocking=True)\n",
    "            mask = data[\"mask\"].to(CFG.device,non_blocking=True)\n",
    "            outputs=self.model(ids,mask)\n",
    "            outputs = outputs[\"logits\"].squeeze(-1)\n",
    "            targets = data[\"targets\"].to(CFG.device,non_blocking=True)\n",
    "            loss=self.loss_fn(outputs,targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            final_loss += loss.item()\n",
    "        return final_loss/len(data_loader)\n",
    "    \n",
    "    def validate(self,data_loader):\n",
    "        self.model.eval()\n",
    "        final_loss=0\n",
    "        for data in tqdm(data_loader):\n",
    "            with torch.no_grad():\n",
    "                ids = data[\"ids\"].to(CFG.device,non_blocking=True)\n",
    "                mask = data[\"mask\"].to(CFG.device,non_blocking=True)\n",
    "                outputs=self.model(ids,mask)\n",
    "                outputs = outputs[\"logits\"].squeeze(-1)\n",
    "                targets = data[\"targets\"].to(CFG.device,non_blocking=True)\n",
    "                loss=self.loss_fn(outputs,targets)\n",
    "                final_loss += loss.item()\n",
    "        return final_loss/len(data_loader)\n",
    "\n",
    "def loop_train(train,fold=0,save_model=True):\n",
    "    # Dataset\n",
    "    train_loader,valid_loader=get_dataloader(train,fold)\n",
    "    model=get_model()\n",
    "    model.to(CFG.device)\n",
    "\n",
    "    # model folder\n",
    "    if os.path.isdir(CFG.model) == False:\n",
    "        os.makedirs(CFG.model)\n",
    "\n",
    "    # Model,Optimizer, scheduler, engine\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,patience=3,threshold=1e-5,mode=\"min\",verbose=True)\n",
    "    engine=Engine(model,optimizer,scheduler)\n",
    "    best_loss=np.inf\n",
    "    early_stopping=10\n",
    "    early_stopping_cnt=0\n",
    "\n",
    "    for epoch in range(CFG.epoch):\n",
    "        train_loss=engine.train(train_loader)\n",
    "        valid_loss=engine.validate(valid_loader)\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss<best_loss:\n",
    "            best_loss=valid_loss\n",
    "            torch.save(model.state_dict(),f\"{CFG.model}/model_{CFG.model}_fold_{fold}.bin\")\n",
    "            print(f\"fold={fold}, epoch={epoch}, train_loss={train_loss:.6f}, valid_loss={valid_loss:.6f}\")    \n",
    "            early_stopping_cnt=0\n",
    "        else:\n",
    "            early_stopping_cnt+=1\n",
    "        if early_stopping_cnt>early_stopping:\n",
    "            break\n",
    "\n",
    "    print(f\"fold={fold}, best val loss={best_loss}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfg import get_cfg\n",
    "CFG=get_cfg()\n",
    "\n",
    "import numpy as np\n",
    "from model import get_model,loss_fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataloader import get_dataset_test,get_dataloader_test\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict(model,dataloader):\n",
    "    preds=[]\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            ids = data[\"ids\"].to(CFG.device,non_blocking=True)\n",
    "            mask = data[\"mask\"].to(CFG.device,non_blocking=True)\n",
    "            outputs=model(ids,mask)\n",
    "            outputs = outputs[\"logits\"].squeeze(-1)\n",
    "            preds.append(outputs.cpu().detach().numpy())\n",
    "        preds=np.concatenate(preds)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def loop_predict(test,fold=0,save_model=True):\n",
    "    # Dataset\n",
    "    test_loader=get_dataloader_test(test)\n",
    "\n",
    "    all_preds=[]\n",
    "    for pth in CFG.fold:\n",
    "        model=get_model()\n",
    "        model.to(CFG.device)\n",
    "        model.eval()\n",
    "        path=f\"{CFG.model}/model_{CFG.model}_fold_{fold}.bin\"\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        preds=predict(model,test_loader)\n",
    "        all_preds.append(preds)\n",
    "    \n",
    "    p=np.array(all_preds).T\n",
    "    preds=np.average(p,axis=1)\n",
    "    test[\"target\"]=preds\n",
    "    return test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                       comment_text  \\\n",
      "159561  ffd2e85b07b3c7e4  \"\\nNo he did not, read it again (I would have ...   \n",
      "159562  ffd72e9766c09c97  \"\\n Auto guides and the motoring press are not...   \n",
      "159563  ffe029a7c79dc7fe  \"\\nplease identify what part of BLP applies be...   \n",
      "159564  ffe897e7f7182c90  Catalan independentism is the social movement ...   \n",
      "159565  ffe8b9316245be30  The numbers in parentheses are the additional ...   \n",
      "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
      "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
      "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
      "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
      "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
      "\n",
      "        toxic  severe_toxic  obscene  threat  insult  identity_hate  target  \\\n",
      "159561      0             0        0       0       0              0       0   \n",
      "159562      0             0        0       0       0              0       0   \n",
      "159563      0             0        0       0       0              0       0   \n",
      "159564      0             0        0       0       0              0       0   \n",
      "159565      0             0        0       0       0              0       0   \n",
      "159566      0             0        0       0       0              0       0   \n",
      "159567      0             0        0       0       0              0       0   \n",
      "159568      0             0        0       0       0              0       0   \n",
      "159569      0             0        0       0       0              0       0   \n",
      "159570      0             0        0       0       0              0       0   \n",
      "\n",
      "        fold  \n",
      "159561     1  \n",
      "159562     1  \n",
      "159563     1  \n",
      "159564     2  \n",
      "159565     4  \n",
      "159566     4  \n",
      "159567     4  \n",
      "159568     0  \n",
      "159569     0  \n",
      "159570     1   (159571, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/943 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\ipofr\\anaconda3\\envs\\tch38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  6%|▋         | 60/943 [00:10<02:36,  5.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c2513e4f3ec1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"comment_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"comment_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-473716de3f75>\u001b[0m in \u001b[0;36mloop_predict\u001b[1;34m(test, fold, save_model)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"{CFG.model}/model_{CFG.model}_fold_{fold}.bin\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mpreds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mall_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-473716de3f75>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"logits\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = get_data()\n",
    "print(train.tail(10),train.shape)\n",
    "\n",
    "if CFG._train:\n",
    "    for fold in CFG.fold:\n",
    "        print(f\"fold :{fold}\")\n",
    "        loop_train(train=train,fold=fold,save_model=True)\n",
    "\n",
    "if CFG._predict:\n",
    "    test = get_data_test()\n",
    "    result = loop_predict(test=test)\n",
    "    result.columns=[\"comment_id\",\"text\",\"score\"]\n",
    "    result[[\"comment_id\",\"score\"]].to_csv(\"submission.csv\",index=False)\n",
    "    result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "318cf3de6d0a79980e53c1022e9a9bd2b66c638045de2aca848fbac357dae712"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tch38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
